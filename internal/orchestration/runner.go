package orchestration

import (
	"context"
	"fmt"
	"math"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/spboyer/waza/internal/cache"
	"github.com/spboyer/waza/internal/config"
	"github.com/spboyer/waza/internal/dataset"
	"github.com/spboyer/waza/internal/execution"
	"github.com/spboyer/waza/internal/graders"
	"github.com/spboyer/waza/internal/hooks"
	"github.com/spboyer/waza/internal/models"
	"github.com/spboyer/waza/internal/statistics"
	"github.com/spboyer/waza/internal/template"
	"github.com/spboyer/waza/internal/transcript"
	"github.com/spboyer/waza/internal/utils"
)

// TestRunner orchestrates the execution of tests
type TestRunner struct {
	cfg     *config.BenchmarkConfig
	engine  execution.AgentEngine
	verbose bool

	// Task filtering
	taskFilters []string

	// Tag filtering for tasks
	tagFilters []string

	// Result caching
	cache *cache.Cache

	// Lifecycle hooks
	hookRunner *hooks.Runner

	// Progress tracking
	progressMu sync.Mutex
	listeners  []ProgressListener
}

// ProgressListener receives progress updates
type ProgressListener func(event ProgressEvent)

// EventType represents the type of progress event
type EventType string

// EventType constants
const (
	EventBenchmarkStart    EventType = "benchmark_start"
	EventBenchmarkComplete EventType = "benchmark_complete"
	EventBenchmarkStopped  EventType = "benchmark_stopped"
	EventTestStart         EventType = "test_start"
	EventTestComplete      EventType = "test_complete"
	EventTestCached        EventType = "test_cached"
	EventRunStart          EventType = "run_start"
	EventRunComplete       EventType = "run_complete"
	EventAgentPrompt       EventType = "agent_prompt"
	EventAgentResponse     EventType = "agent_response"
	EventGraderResult      EventType = "grader_result"
)

// ProgressEvent represents a progress update
type ProgressEvent struct {
	EventType  EventType
	TestName   string
	TestNum    int
	TotalTests int
	RunNum     int
	TotalRuns  int
	Status     models.Status
	DurationMs int64
	Details    map[string]any
}

// RunnerOption configures a TestRunner.
type RunnerOption func(*TestRunner)

// WithTaskFilters sets glob patterns used to filter test cases by DisplayName or TestID.
func WithTaskFilters(patterns ...string) RunnerOption {
	return func(r *TestRunner) {
		r.taskFilters = patterns
	}
}

func WithTagFilters(patterns ...string) RunnerOption {
	return func(r *TestRunner) {
		r.tagFilters = patterns
	}
}

// WithCache enables result caching
func WithCache(c *cache.Cache) RunnerOption {
	return func(r *TestRunner) {
		r.cache = c
	}
}

// NewTestRunner creates a new test runner
func NewTestRunner(cfg *config.BenchmarkConfig, engine execution.AgentEngine, opts ...RunnerOption) *TestRunner {
	r := &TestRunner{
		cfg:       cfg,
		engine:    engine,
		verbose:   cfg.Verbose(),
		listeners: []ProgressListener{},
	}
	for _, o := range opts {
		o(r)
	}
	return r
}

// OnProgress registers a progress listener
func (r *TestRunner) OnProgress(listener ProgressListener) {
	r.progressMu.Lock()
	defer r.progressMu.Unlock()
	r.listeners = append(r.listeners, listener)
}

// testOutcomeDetails extracts score and duration from a TestOutcome for inclusion
// in EventTestComplete Details.
func testOutcomeDetails(o *models.TestOutcome) map[string]any {
	score := 0.0
	durationMs := int64(0)
	if o.Stats != nil {
		score = o.Stats.AvgScore
		durationMs = o.Stats.AvgDurationMs
	}
	return map[string]any{
		"score":       score,
		"duration_ms": durationMs,
	}
}

func (r *TestRunner) notifyProgress(event ProgressEvent) {
	r.progressMu.Lock()
	listeners := make([]ProgressListener, len(r.listeners))
	copy(listeners, r.listeners)
	r.progressMu.Unlock()

	for _, listener := range listeners {
		listener(event)
	}
}

// RunBenchmark executes the entire benchmark
// If Baseline is enabled, runs twice: skills-enabled and skills-disabled
func (r *TestRunner) RunBenchmark(ctx context.Context) (*models.EvaluationOutcome, error) {
	spec := r.cfg.Spec()

	if spec.Baseline {
		return r.runBaselineComparison(ctx)
	}

	return r.runNormalBenchmark(ctx)
}

// runNormalBenchmark executes a normal single-pass evaluation
func (r *TestRunner) runNormalBenchmark(ctx context.Context) (*models.EvaluationOutcome, error) {
	startTime := time.Now()

	// Initialize engine
	if err := r.engine.Initialize(ctx); err != nil {
		return nil, fmt.Errorf("failed to initialize engine: %w", err)
	}
	defer func() {
		if err := r.engine.Shutdown(ctx); err != nil {
			fmt.Printf("warning: failed to shutdown engine: %v\n", err)
		}
	}()

	// Set up hooks runner
	spec := r.cfg.Spec()
	r.hookRunner = &hooks.Runner{Verbose: r.verbose}

	// Run after_run hooks on exit (even on error)
	defer func() {
		if len(spec.Hooks.AfterRun) > 0 {
			if err := r.hookRunner.Execute(ctx, "after_run", spec.Hooks.AfterRun); err != nil {
				fmt.Printf("[WARN] after_run hook error: %v\n", err)
			}
		}
	}()

	// Run before_run hooks
	if len(spec.Hooks.BeforeRun) > 0 {
		if err := r.hookRunner.Execute(ctx, "before_run", spec.Hooks.BeforeRun); err != nil {
			return nil, fmt.Errorf("before_run hook failed: %w", err)
		}
	}

	// Preflight check: validate required skills
	if err := r.validateRequiredSkills(); err != nil {
		return nil, err
	}

	// Load test cases
	testCases, err := r.loadTestCases()
	if err != nil {
		return nil, fmt.Errorf("failed to load test cases: %w", err)
	}

	// Apply task/tag filters
	if len(r.taskFilters) > 0 || len(r.tagFilters) > 0 {
		testCases, err = FilterTestCases(testCases, r.taskFilters, r.tagFilters)
		if err != nil {
			return nil, fmt.Errorf("task/tag filter error: %w", err)
		}
		fmt.Printf("Task and tag filters matched %d test(s):\n", len(testCases))
		for _, tc := range testCases {
			fmt.Printf("  • %s (%s)\n", tc.DisplayName, tc.TestID)
		}
		fmt.Println()
	}

	if len(testCases) == 0 {
		return nil, fmt.Errorf("no test cases found")
	}

	r.notifyProgress(ProgressEvent{
		EventType:  EventBenchmarkStart,
		TotalTests: len(testCases),
	})

	// Execute tests
	var testOutcomes []models.TestOutcome

	// Now that CopilotEngine is concurrency-safe (protected by mutex),
	// we can safely use concurrent execution when configured
	if spec.Config.Concurrent {
		testOutcomes = r.runConcurrent(ctx, testCases)
	} else {
		testOutcomes = r.runSequential(ctx, testCases)
	}

	// Compute statistics
	outcome := r.buildOutcome(testOutcomes, startTime)

	r.notifyProgress(ProgressEvent{
		EventType:  EventBenchmarkComplete,
		DurationMs: time.Since(startTime).Milliseconds(),
	})

	return outcome, nil
}

// runBaselineComparison orchestrates A/B testing: skills-enabled vs skills-disabled
func (r *TestRunner) runBaselineComparison(ctx context.Context) (*models.EvaluationOutcome, error) {
	spec := r.cfg.Spec()

	// Validation: eval must have skills configured
	if len(spec.Config.SkillPaths) == 0 && len(spec.Config.RequiredSkills) == 0 {
		fmt.Println("[WARN] --baseline specified but eval has no skills configured (skill_directories, required_skills empty). Skipping baseline comparison.")
		return r.runNormalBenchmark(ctx)
	}

	// PASS 1: Skills-Enabled
	fmt.Println("\n════════════════════════════════════════════════════════════════")
	fmt.Println("PASS 1: Skills-Enabled Run")
	fmt.Println("════════════════════════════════════════════════════════════════")
	outcomesWithSkills, err := r.runNormalBenchmark(ctx)
	if err != nil {
		return nil, fmt.Errorf("skills-enabled run failed: %w", err)
	}

	// PASS 2: Skills Disabled (baseline)
	savedSkillPaths := spec.Config.SkillPaths
	savedRequiredSkills := spec.Config.RequiredSkills
	spec.Config.SkillPaths = []string{}
	spec.Config.RequiredSkills = []string{}
	defer func() {
		spec.Config.SkillPaths = savedSkillPaths
		spec.Config.RequiredSkills = savedRequiredSkills
	}()

	fmt.Println("\n════════════════════════════════════════════════════════════════")
	fmt.Println("PASS 2: Skills Baseline (skills stripped)")
	fmt.Println("════════════════════════════════════════════════════════════════")
	outcomesWithoutSkills, err := r.runNormalBenchmark(ctx)
	if err != nil {
		return nil, fmt.Errorf("baseline run (skills disabled) failed: %w", err)
	}

	// Restore skills before merging
	spec.Config.SkillPaths = savedSkillPaths
	spec.Config.RequiredSkills = savedRequiredSkills

	// PASS 3: Compare and merge results
	return r.mergeBaselineOutcomes(outcomesWithSkills, outcomesWithoutSkills)
}

// mergeBaselineOutcomes pairs task results and computes skill impact
func (r *TestRunner) mergeBaselineOutcomes(
	withSkills, withoutSkills *models.EvaluationOutcome,
) (*models.EvaluationOutcome, error) {

	// Build maps: TestID → TestOutcome for quick lookup
	withMap := make(map[string]*models.TestOutcome)
	withoutMap := make(map[string]*models.TestOutcome)

	for i := range withSkills.TestOutcomes {
		withMap[withSkills.TestOutcomes[i].TestID] = &withSkills.TestOutcomes[i]
	}
	for i := range withoutSkills.TestOutcomes {
		withoutMap[withoutSkills.TestOutcomes[i].TestID] = &withoutSkills.TestOutcomes[i]
	}

	// Merge: for each task, compute skill_impact
	for testID, withTo := range withMap {
		withoutTo, ok := withoutMap[testID]
		if !ok {
			return nil, fmt.Errorf("baseline mismatch: task %q present in skills-enabled but not baseline", testID)
		}

		withTo.SkillImpact = computeSkillImpact(withTo, withoutTo)
	}

	// Check for extra tasks in baseline
	for testID := range withoutMap {
		if _, ok := withMap[testID]; !ok {
			return nil, fmt.Errorf("baseline mismatch: task %q present in baseline but not skills-enabled", testID)
		}
	}

	// Print comparison report
	r.printSkillImpactReport(withSkills, withoutSkills)

	// Return merged outcome (use withSkills as the primary result)
	withSkills.IsBaseline = true
	withSkills.BaselineOutcome = withoutSkills
	return withSkills, nil
}

// computeSkillImpact calculates per-task impact metric
func computeSkillImpact(withSkills, without *models.TestOutcome) *models.SkillImpactMetric {
	passRateWith := computePassRate(withSkills)
	passRateWithout := computePassRate(without)

	delta := passRateWith - passRateWithout

	// Compute % improvement (with div-by-zero guard)
	denom := math.Max(passRateWithout, 0.01)
	percentImprovement := (delta / denom) * 100.0

	return &models.SkillImpactMetric{
		PassRateWithSkills: passRateWith,
		PassRateBaseline:   passRateWithout,
		Delta:              delta,
		PercentChange:      percentImprovement,
	}
}

func computePassRate(outcome *models.TestOutcome) float64 {
	if outcome.Stats != nil {
		return outcome.Stats.PassRate
	}
	// Fallback: compute from runs when stats haven't been populated yet
	if len(outcome.Runs) == 0 {
		return 0.0
	}
	passed := 0
	for _, r := range outcome.Runs {
		if r.Status == models.StatusPassed {
			passed++
		}
	}
	return float64(passed) / float64(len(outcome.Runs))
}

// printSkillImpactReport prints the A/B comparison summary
func (r *TestRunner) printSkillImpactReport(withSkills, withoutSkills *models.EvaluationOutcome) {
	fmt.Println("\n════════════════════════════════════════════════════════════════")
	fmt.Println("SKILL IMPACT ANALYSIS")
	fmt.Println("════════════════════════════════════════════════════════════════")

	withPassRate := withSkills.Digest.SuccessRate
	withoutPassRate := withoutSkills.Digest.SuccessRate
	delta := withPassRate - withoutPassRate

	fmt.Printf("Overall Performance Delta:\n")
	fmt.Printf("  With Skills:    %.1f%% (%d/%d tasks passed)\n",
		withPassRate*100, withSkills.Digest.Succeeded, withSkills.Digest.TotalTests)
	fmt.Printf("  Without Skills: %.1f%% (%d/%d tasks passed)\n",
		withoutPassRate*100, withoutSkills.Digest.Succeeded, withoutSkills.Digest.TotalTests)

	if delta > 0 {
		fmt.Printf("  Impact:         +%.1f percentage points\n\n", delta*100)
	} else if delta < 0 {
		fmt.Printf("  Impact:         %.1f percentage points\n\n", delta*100)
	} else {
		fmt.Printf("  Impact:         no change\n\n")
	}

	fmt.Println("Per-Task Breakdown:")
	improved := 0
	regressed := 0
	neutral := 0

	for i := range withSkills.TestOutcomes {
		to := &withSkills.TestOutcomes[i]
		if to.SkillImpact == nil {
			continue
		}

		impact := to.SkillImpact
		status := "[NEUTRAL]"
		if impact.Delta > 0 {
			status = "[IMPROVED]"
			improved++
		} else if impact.Delta < 0 {
			status = "[REGRESSED]"
			regressed++
		} else {
			neutral++
		}

		fmt.Printf("  • %-30s %s  %.0f%% → %.0f%% (%+.0fpp)\n",
			to.DisplayName,
			status,
			impact.PassRateBaseline*100,
			impact.PassRateWithSkills*100,
			impact.Delta*100,
		)
	}

	fmt.Println()
	if delta > 0 {
		fmt.Printf("Verdict: Skills have POSITIVE IMPACT (improved %d/%d tasks)\n", improved, len(withSkills.TestOutcomes))
	} else if delta < 0 {
		fmt.Printf("Verdict: Skills have NEGATIVE IMPACT (regressed %d/%d tasks)\n", regressed, len(withSkills.TestOutcomes))
	} else {
		fmt.Printf("Verdict: Skills have NEUTRAL IMPACT (no net change)\n")
	}
	fmt.Println("════════════════════════════════════════════════════════════════")
}

func (r *TestRunner) loadTestCases() ([]*models.TestCase, error) {
	spec := r.cfg.Spec()

	// CSV dataset path: generate tasks from CSV rows
	if spec.TasksFrom != "" {
		return r.loadTestCasesFromCSV()
	}

	// Fall through to existing Tasks []string behavior
	return r.loadTestCasesFromFiles()
}

// loadTestCasesFromCSV generates in-memory TestCases from CSV rows.
func (r *TestRunner) loadTestCasesFromCSV() ([]*models.TestCase, error) {
	spec := r.cfg.Spec()

	// Resolve CSV path relative to spec directory
	csvPath := spec.TasksFrom
	baseDir := r.cfg.SpecDir()
	if baseDir == "" {
		baseDir = "."
	}
	if !filepath.IsAbs(csvPath) {
		csvPath = filepath.Join(baseDir, csvPath)
	}

	// Path containment: CSV must resolve within spec directory
	absBaseDir, err := filepath.Abs(baseDir)
	if err != nil {
		return nil, fmt.Errorf("resolving spec directory: %w", err)
	}
	absCSVPath, err := filepath.Abs(csvPath)
	if err != nil {
		return nil, fmt.Errorf("resolving CSV path: %w", err)
	}
	if !strings.HasPrefix(absCSVPath, absBaseDir+string(filepath.Separator)) {
		return nil, fmt.Errorf("tasks_from path %q escapes spec directory", spec.TasksFrom)
	}

	// Validate and load CSV with optional range filtering
	var rows []dataset.Row
	if spec.Range != [2]int{} {
		if spec.Range[0] <= 0 || spec.Range[1] <= 0 {
			return nil, fmt.Errorf("invalid range: both values must be > 0, got [%d, %d]", spec.Range[0], spec.Range[1])
		}
		if spec.Range[0] > spec.Range[1] {
			return nil, fmt.Errorf("invalid range: start (%d) must be <= end (%d)", spec.Range[0], spec.Range[1])
		}
		rows, err = dataset.LoadCSVRange(csvPath, spec.Range[0], spec.Range[1])
	} else {
		rows, err = dataset.LoadCSV(csvPath)
	}
	if err != nil {
		return nil, fmt.Errorf("loading CSV dataset: %w", err)
	}

	// Build template context for resolving templates
	now := time.Now()
	baseCtx := &template.Context{
		JobID:     fmt.Sprintf("run-%d", now.Unix()),
		Timestamp: now.Format(time.RFC3339),
		Vars:      make(map[string]string),
	}

	// Merge spec.Inputs as base variables
	for k, v := range spec.Inputs {
		baseCtx.Vars[k] = v
	}

	testCases := make([]*models.TestCase, 0, len(rows))
	for i, row := range rows {
		rowNum := i + 1

		// Determine TestID: prefer "id" column, then "name", then "row-N"
		testID := fmt.Sprintf("row-%d", rowNum)
		if v, ok := row["id"]; ok && v != "" {
			testID = v
		} else if v, ok := row["name"]; ok && v != "" {
			testID = v
		}

		// Determine DisplayName: prefer "name" column, then "row-N"
		displayName := fmt.Sprintf("row-%d", rowNum)
		if v, ok := row["name"]; ok && v != "" {
			displayName = v
		}

		// Build per-row template context: inputs + CSV row (CSV overrides inputs on conflict)
		rowCtx := &template.Context{
			JobID:     baseCtx.JobID,
			TaskName:  displayName,
			Iteration: 0,
			Attempt:   0,
			Timestamp: baseCtx.Timestamp,
			Vars:      make(map[string]string),
		}
		for k, v := range spec.Inputs {
			rowCtx.Vars[k] = v
		}
		for k, v := range row {
			rowCtx.Vars[k] = v
		}

		// Resolve prompt: use "prompt" column if present, otherwise empty
		prompt := row["prompt"]
		if strings.Contains(prompt, "{{") {
			prompt, err = template.Render(prompt, rowCtx)
			if err != nil {
				return nil, fmt.Errorf("resolving prompt template for row %d: %w", rowNum, err)
			}
		}

		tc := &models.TestCase{
			TestID:      testID,
			DisplayName: displayName,
			Stimulus: models.TestStimulus{
				Message: prompt,
			},
		}
		testCases = append(testCases, tc)
	}

	return testCases, nil
}

// loadTestCasesFromFiles loads test cases from YAML files via glob patterns.
func (r *TestRunner) loadTestCasesFromFiles() ([]*models.TestCase, error) {
	spec := r.cfg.Spec()

	// Get base directory for test file resolution (spec directory)
	baseDir := r.cfg.SpecDir()
	if baseDir == "" {
		baseDir = "."
	}

	// Resolve test file patterns relative to the spec directory
	testFiles := []string{}
	for _, pattern := range spec.Tasks {
		fullPattern := filepath.Join(baseDir, pattern)
		matches, err := filepath.Glob(fullPattern)
		if err != nil {
			return nil, err
		}
		testFiles = append(testFiles, matches...)
	}

	if len(testFiles) == 0 {
		return nil, fmt.Errorf("no test files matched patterns: %v in directory: %s", spec.Tasks, baseDir)
	}

	var testCases []*models.TestCase
	for _, path := range testFiles {
		tc, err := models.LoadTestCase(path)
		if err != nil {
			return nil, fmt.Errorf("failed to load test case %s: %w", path, err)
		}
		// Only include active test cases
		// LoadTestCase defaults Active to true (nil case), so include nil or explicitly true
		if tc.Active == nil || *tc.Active {
			testCases = append(testCases, tc)
		}
	}

	return testCases, nil
}

// validateRequiredSkills performs preflight validation that all required skills are present.
func (r *TestRunner) validateRequiredSkills() error {
	spec := r.cfg.Spec()

	// If no required skills specified, skip validation
	if len(spec.Config.RequiredSkills) == 0 {
		return nil
	}

	// Get base directory for path resolution
	baseDir := r.cfg.SpecDir()
	if baseDir == "" {
		baseDir = "."
	}

	// Resolve skill paths
	resolvedPaths := utils.ResolvePaths(spec.Config.SkillPaths, baseDir)

	// If required skills specified but no skill directories, that's an error
	if len(resolvedPaths) == 0 {
		return fmt.Errorf("required_skills specified but no skill_directories configured")
	}

	// Discover skills in the specified directories
	discoveredSkills, err := discoverSkills(resolvedPaths)
	if err != nil {
		return fmt.Errorf("discovering skills: %w", err)
	}

	// Validate that all required skills were found
	if err := validateRequiredSkills(spec.Config.RequiredSkills, discoveredSkills, resolvedPaths); err != nil {
		return fmt.Errorf("skill validation failed:\n%w", err)
	}

	if r.verbose {
		fmt.Printf("✓ Required skills validation passed (%d/%d skills found)\n\n",
			len(spec.Config.RequiredSkills), len(spec.Config.RequiredSkills))
	}

	return nil
}

func (r *TestRunner) runSequential(ctx context.Context, testCases []*models.TestCase) []models.TestOutcome {
	outcomes := make([]models.TestOutcome, 0, len(testCases))
	spec := r.cfg.Spec()

	for i, tc := range testCases {
		// Check if we should stop on error
		if spec.Config.StopOnError && i > 0 {
			// Check if any previous test failed or had an error
			for _, prevResult := range outcomes {
				if prevResult.Status != models.StatusPassed {
					r.notifyProgress(ProgressEvent{
						EventType: EventBenchmarkStopped,
						Details:   map[string]any{"reason": "fail_fast enabled and previous test failed"},
					})
					// Skip remaining tests
					return outcomes
				}
			}
		}

		// Run before_task hooks
		if r.hookRunner != nil && len(spec.Hooks.BeforeTask) > 0 {
			if err := r.hookRunner.Execute(ctx, "before_task", spec.Hooks.BeforeTask); err != nil {
				// before_task failure with error_on_fail: mark task as failed and skip
				outcomes = append(outcomes, models.TestOutcome{
					TestID:      tc.TestID,
					DisplayName: tc.DisplayName,
					Status:      models.StatusFailed,
					Runs:        []models.RunResult{},
				})
				r.notifyProgress(ProgressEvent{
					EventType:  EventTestComplete,
					TestName:   tc.DisplayName,
					TestNum:    i + 1,
					TotalTests: len(testCases),
					Status:     models.StatusFailed,
					Details:    map[string]any{"score": 0.0, "duration_ms": int64(0)},
				})
				continue
			}
		}

		r.notifyProgress(ProgressEvent{
			EventType:  EventTestStart,
			TestName:   tc.DisplayName,
			TestNum:    i + 1,
			TotalTests: len(testCases),
		})

		taskStart := time.Now()
		outcome, wasCached := r.runTest(ctx, tc, i+1, len(testCases))
		r.writeTaskTranscript(tc, outcome, taskStart)
		outcomes = append(outcomes, outcome)

		// Run after_task hooks
		if r.hookRunner != nil && len(spec.Hooks.AfterTask) > 0 {
			if err := r.hookRunner.Execute(ctx, "after_task", spec.Hooks.AfterTask); err != nil {
				fmt.Printf("[WARN] after_task hook error for %s: %v\n", tc.DisplayName, err)
			}
		}

		if wasCached {
			// Emit cached event instead of complete
			r.notifyProgress(ProgressEvent{
				EventType:  EventTestCached,
				TestName:   tc.DisplayName,
				TestNum:    i + 1,
				TotalTests: len(testCases),
				Status:     outcome.Status,
			})
		} else {
			r.notifyProgress(ProgressEvent{
				EventType:  EventTestComplete,
				TestName:   tc.DisplayName,
				TestNum:    i + 1,
				TotalTests: len(testCases),
				Status:     outcome.Status,
				Details:    testOutcomeDetails(&outcome),
			})
		}
	}

	return outcomes
}

func (r *TestRunner) runConcurrent(ctx context.Context, testCases []*models.TestCase) []models.TestOutcome {
	// Simple concurrent implementation
	spec := r.cfg.Spec()
	workers := spec.Config.Workers
	if workers <= 0 {
		workers = 4
	}

	type result struct {
		index   int
		outcome models.TestOutcome
	}

	resultChan := make(chan result, len(testCases))
	semaphore := make(chan struct{}, workers)

	var wg sync.WaitGroup

	for i, tc := range testCases {
		wg.Add(1)
		go func(idx int, test *models.TestCase) {
			defer wg.Done()

			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// Run before_task hooks
			if r.hookRunner != nil && len(spec.Hooks.BeforeTask) > 0 {
				if err := r.hookRunner.Execute(ctx, "before_task", spec.Hooks.BeforeTask); err != nil {
					resultChan <- result{index: idx, outcome: models.TestOutcome{
						TestID:      test.TestID,
						DisplayName: test.DisplayName,
						Status:      models.StatusFailed,
						Runs:        []models.RunResult{},
					}}
					r.notifyProgress(ProgressEvent{
						EventType:  EventTestComplete,
						TestName:   test.DisplayName,
						TestNum:    idx + 1,
						TotalTests: len(testCases),
						Status:     models.StatusFailed,
						Details:    map[string]any{"score": 0.0, "duration_ms": int64(0)},
					})
					return
				}
			}

			r.notifyProgress(ProgressEvent{
				EventType:  EventTestStart,
				TestName:   test.DisplayName,
				TestNum:    idx + 1,
				TotalTests: len(testCases),
			})

			taskStart := time.Now()
			outcome, wasCached := r.runTest(ctx, test, idx+1, len(testCases))
			r.writeTaskTranscript(test, outcome, taskStart)
			resultChan <- result{index: idx, outcome: outcome}

			// Run after_task hooks
			if r.hookRunner != nil && len(spec.Hooks.AfterTask) > 0 {
				if err := r.hookRunner.Execute(ctx, "after_task", spec.Hooks.AfterTask); err != nil {
					fmt.Printf("[WARN] after_task hook error for %s: %v\n", test.DisplayName, err)
				}
			}

			if wasCached {
				r.notifyProgress(ProgressEvent{
					EventType:  EventTestCached,
					TestName:   test.DisplayName,
					TestNum:    idx + 1,
					TotalTests: len(testCases),
					Status:     outcome.Status,
				})
			} else {
				r.notifyProgress(ProgressEvent{
					EventType:  EventTestComplete,
					TestName:   test.DisplayName,
					TestNum:    idx + 1,
					TotalTests: len(testCases),
					Status:     outcome.Status,
					Details:    testOutcomeDetails(&outcome),
				})
			}
		}(i, tc)
	}

	go func() {
		wg.Wait()
		close(resultChan)
	}()

	// Collect results
	results := make([]models.TestOutcome, len(testCases))
	for res := range resultChan {
		results[res.index] = res.outcome
	}

	return results
}

func (r *TestRunner) runTest(ctx context.Context, tc *models.TestCase, testNum, totalTests int) (models.TestOutcome, bool) {
	spec := r.cfg.Spec()

	// Check cache if enabled
	if r.cache != nil {
		cacheKey, err := cache.CacheKey(spec, tc, r.cfg.FixtureDir())
		if err == nil {
			if cachedOutcome, found := r.cache.Get(cacheKey); found {
				// Return cached outcome with cached flag
				return *cachedOutcome, true
			}
			// Run the test and cache the result
			outcome := r.runTestUncached(ctx, tc, testNum, totalTests)
			// Store in cache and log any failures
			if err := r.cache.Put(cacheKey, &outcome); err != nil {
				fmt.Fprintf(os.Stderr, "[WARN] Failed to write cache for test %q: %v\n", tc.DisplayName, err)
			}
			return outcome, false
		}
	}

	// No cache or cache key generation failed
	return r.runTestUncached(ctx, tc, testNum, totalTests), false
}

func (r *TestRunner) writeTaskTranscript(tc *models.TestCase, outcome models.TestOutcome, startTime time.Time) {
	transcriptDir := r.cfg.TranscriptDir()
	if transcriptDir == "" {
		return
	}

	taskTranscript := transcript.BuildTaskTranscript(tc, outcome, startTime)
	if _, err := transcript.Write(transcriptDir, taskTranscript); err != nil {
		fmt.Fprintf(os.Stderr, "[WARN] Failed to write transcript for %q: %v\n", tc.DisplayName, err)
	}
}

func (r *TestRunner) runTestUncached(ctx context.Context, tc *models.TestCase, testNum, totalTests int) models.TestOutcome {
	spec := r.cfg.Spec()
	runsPerTest := spec.Config.RunsPerTest
	maxAttempts := spec.Config.MaxAttempts
	if maxAttempts < 1 {
		maxAttempts = 1
	}

	runs := make([]models.RunResult, 0, runsPerTest)

	for runNum := 1; runNum <= runsPerTest; runNum++ {
		r.notifyProgress(ProgressEvent{
			EventType:  EventRunStart,
			TestName:   tc.DisplayName,
			TestNum:    testNum,
			TotalTests: totalTests,
			RunNum:     runNum,
			TotalRuns:  runsPerTest,
		})

		var run models.RunResult
		for attempt := 1; attempt <= maxAttempts; attempt++ {
			run = r.executeRun(ctx, tc, runNum)
			run.Attempts = attempt

			// If all graders passed or this is an infrastructure error, stop retrying
			if run.Status == models.StatusPassed || run.Status == models.StatusError {
				break
			}

			// If more attempts remain, log the retry
			if attempt < maxAttempts && r.verbose {
				fmt.Printf("[RETRY] %s run %d: attempt %d/%d failed, retrying\n",
					tc.DisplayName, runNum, attempt, maxAttempts)
			}
		}

		// Surface errors even in non-verbose mode because they're critical for understanding test failures
		if run.ErrorMsg != "" && !r.verbose {
			fmt.Printf("[ERROR] %s\n\n", run.ErrorMsg)
		}

		runs = append(runs, run)

		r.notifyProgress(ProgressEvent{
			EventType:  EventRunComplete,
			TestName:   tc.DisplayName,
			TestNum:    testNum,
			TotalTests: totalTests,
			RunNum:     runNum,
			TotalRuns:  runsPerTest,
			Status:     run.Status,
			DurationMs: run.DurationMs,
		})
	}

	// Compute test statistics
	stats := r.computeTestStats(runs)

	// Determine overall status
	status := models.StatusPassed
	for _, run := range runs {
		if run.Status != models.StatusPassed {
			status = models.StatusFailed
			break
		}
	}

	return models.TestOutcome{
		TestID:      tc.TestID,
		DisplayName: tc.DisplayName,
		Group:       r.resolveGroup(),
		Status:      status,
		Runs:        runs,
		Stats:       stats,
	}
}

func (r *TestRunner) executeRun(ctx context.Context, tc *models.TestCase, runNum int) models.RunResult {
	startTime := time.Now()

	// Prepare execution request
	req := r.buildExecutionRequest(tc)

	// Emit agent prompt event before execution
	if r.verbose {
		r.notifyProgress(ProgressEvent{
			EventType: EventAgentPrompt,
			TestName:  tc.DisplayName,
			Details:   map[string]any{"message": req.Message},
		})
	}

	// Execute
	resp, err := r.engine.Execute(ctx, req)
	if err != nil {
		return models.RunResult{
			RunNumber:  runNum,
			Status:     models.StatusError,
			DurationMs: time.Since(startTime).Milliseconds(),
			ErrorMsg:   err.Error(),
		}
	}

	// Emit agent response event after execution
	if r.verbose {
		r.notifyProgress(ProgressEvent{
			EventType: EventAgentResponse,
			TestName:  tc.DisplayName,
			Details: map[string]any{
				"error":      resp.ErrorMsg,
				"output":     resp.FinalOutput,
				"transcript": r.buildTranscript(resp),
				"tool_calls": len(resp.ToolCalls),
			},
		})
	}

	// Build validation context
	vCtx := r.buildGraderContext(tc, resp)

	gradersResults, err := r.runGraders(ctx, tc, vCtx)

	if err != nil {
		return models.RunResult{
			RunNumber:  runNum,
			Status:     models.StatusError,
			DurationMs: time.Since(startTime).Milliseconds(),
			ErrorMsg:   "running graders: " + err.Error(),
		}
	}

	// Emit grader result events (sorted for stable output)
	graderNames := make([]string, 0, len(gradersResults))
	for name := range gradersResults {
		graderNames = append(graderNames, name)
	}
	sort.Strings(graderNames)
	for _, name := range graderNames {
		gr := gradersResults[name]
		r.notifyProgress(ProgressEvent{
			EventType:  EventGraderResult,
			TestName:   tc.DisplayName,
			DurationMs: gr.DurationMs,
			Details: map[string]any{
				"grader":      name,
				"grader_type": gr.Type,
				"passed":      gr.Passed,
				"score":       gr.Score,
				"feedback":    gr.Feedback,
			},
		})
	}

	// Determine status
	status := models.StatusPassed
	if resp.ErrorMsg != "" {
		status = models.StatusError
	} else {
		for _, v := range gradersResults {
			if !v.Passed {
				status = models.StatusFailed
				break
			}
		}
	}

	// Build transcript
	transcript := r.buildTranscript(resp)

	return models.RunResult{
		RunNumber:     runNum,
		Status:        status,
		DurationMs:    resp.DurationMs,
		Validations:   gradersResults,
		SessionDigest: r.buildSessionDigest(resp),
		Transcript:    transcript,
		FinalOutput:   resp.FinalOutput,
		ErrorMsg:      resp.ErrorMsg,
	}
}

func (r *TestRunner) buildExecutionRequest(tc *models.TestCase) *execution.ExecutionRequest {
	// Load resource files
	resources := r.loadResources(tc)

	spec := r.cfg.Spec()
	timeout := spec.Config.TimeoutSec
	if tc.TimeoutSec != nil {
		timeout = *tc.TimeoutSec
	}

	// Resolve skill paths relative to spec directory
	resolvedSkillPaths := utils.ResolvePaths(spec.Config.SkillPaths, r.cfg.SpecDir())

	return &execution.ExecutionRequest{
		TestID:     tc.TestID,
		Message:    tc.Stimulus.Message,
		Context:    tc.Stimulus.Metadata,
		Resources:  resources,
		SkillName:  spec.SkillName,
		SkillPaths: resolvedSkillPaths,
		TimeoutSec: timeout,
	}
}

func (r *TestRunner) loadResources(tc *models.TestCase) []execution.ResourceFile {
	var resources []execution.ResourceFile

	// Determine fixture directory (for loading resource files)
	fixtureDir := r.cfg.FixtureDir()
	if tc.ContextRoot != "" {
		fixtureDir = tc.ContextRoot
	}

	for _, ref := range tc.Stimulus.Resources {
		if ref.Body != "" {
			// Inline content
			resources = append(resources, execution.ResourceFile{
				Path:    ref.Location,
				Content: ref.Body,
			})
		} else if ref.Location != "" && fixtureDir != "" {
			// Load from file - validate path to prevent directory traversal
			if filepath.IsAbs(ref.Location) {
				fmt.Fprintf(os.Stderr, "Warning: absolute resource path %q rejected\n", ref.Location)
				continue
			}

			cleanPath := filepath.Clean(ref.Location)
			if strings.Contains(cleanPath, "..") {
				fmt.Fprintf(os.Stderr, "Warning: resource path %q contains '..' and is rejected\n", ref.Location)
				continue
			}

			fullPath := filepath.Join(fixtureDir, cleanPath)

			// Ensure the resolved path is still within fixtureDir
			absFixtureDir, err := filepath.Abs(fixtureDir)
			if err != nil {
				fmt.Fprintf(os.Stderr, "Warning: failed to get absolute path for fixture dir: %v\n", err)
				continue
			}

			absFullPath, err := filepath.Abs(fullPath)
			if err != nil {
				fmt.Fprintf(os.Stderr, "Warning: failed to get absolute path for resource: %v\n", err)
				continue
			}

			if !strings.HasPrefix(absFullPath, absFixtureDir+string(filepath.Separator)) {
				fmt.Fprintf(os.Stderr, "Warning: resource path %q escapes fixture directory\n", ref.Location)
				continue
			}

			content, err := os.ReadFile(fullPath)
			if err != nil {
				// Log error but continue - let the test fail if resource is critical
				fmt.Fprintf(os.Stderr, "Warning: failed to load resource file %s: %v\n", fullPath, err)
				continue
			}
			resources = append(resources, execution.ResourceFile{
				Path:    ref.Location,
				Content: string(content),
			})
		}
	}

	return resources
}

func (r *TestRunner) buildGraderContext(tc *models.TestCase, resp *execution.ExecutionResponse) *graders.Context {
	// Convert events to transcript entries
	var transcript []models.TranscriptEvent
	for _, evt := range resp.Events {
		entry := models.TranscriptEvent{SessionEvent: evt}
		transcript = append(transcript, entry)
	}

	return &graders.Context{
		TestCase:         tc,
		Transcript:       transcript,
		Output:           resp.FinalOutput,
		Outcome:          make(map[string]any),
		DurationMS:       resp.DurationMs,
		Metadata:         make(map[string]any),
		WorkspaceDir:     resp.WorkspaceDir,
		SkillInvocations: resp.SkillInvocations,
		SessionID:        resp.SessionID,
	}
}

func (r *TestRunner) runGraders(ctx context.Context, tc *models.TestCase, gradersContext *graders.Context) (map[string]models.GraderResults, error) {
	graderResults := make(map[string]models.GraderResults)

	// Run global validators
	spec := r.cfg.Spec()
	judgeModel := spec.Config.JudgeModel
	for _, vCfg := range spec.Graders {
		params := vCfg.Parameters
		if judgeModel != "" && vCfg.Kind == models.GraderKindPrompt {
			params = injectJudgeModel(params, judgeModel)
		}
		grader, err := graders.Create(vCfg.Kind, vCfg.Identifier, params)

		if err != nil {
			return nil, err
		}

		result, err := grader.Grade(ctx, gradersContext)

		if err != nil {
			return nil, err
		}

		result.Weight = vCfg.EffectiveWeight()
		graderResults[result.Name] = *result
	}

	// Run test-specific validators
	for _, vCfg := range tc.Validators {
		kind := vCfg.Kind
		if kind == "" {
			return nil, fmt.Errorf("no kind associated with grader %s", vCfg.Identifier)
		}

		params := vCfg.Parameters
		if params == nil {
			params = make(map[string]any)
		}
		if len(vCfg.Checks) > 0 {
			params["assertions"] = vCfg.Checks
		}
		if judgeModel != "" && kind == models.GraderKindPrompt {
			params = injectJudgeModel(params, judgeModel)
		}

		grader, err := graders.Create(kind, vCfg.Identifier, params)

		if err != nil {
			return nil, fmt.Errorf("failed to create grader %s: %w", vCfg.Identifier, err)
		}

		result, err := grader.Grade(ctx, gradersContext)

		if err != nil {
			return nil, fmt.Errorf("failed to run grader %s: %w", vCfg.Identifier, err)
		}

		w := vCfg.Weight
		if w <= 0 {
			w = 1.0
		}
		result.Weight = w
		graderResults[result.Name] = *result
	}

	return graderResults, nil
}

// injectJudgeModel returns a copy of params with the "model" key set to judgeModel.
func injectJudgeModel(params map[string]any, judgeModel string) map[string]any {
	merged := make(map[string]any, len(params)+1)
	for k, v := range params {
		merged[k] = v
	}
	merged["model"] = judgeModel
	return merged
}

func (r *TestRunner) buildSessionDigest(resp *execution.ExecutionResponse) models.SessionDigest {
	toolsUsed := make([]string, 0)
	for _, call := range resp.ToolCalls {
		toolsUsed = append(toolsUsed, call.Name)
	}

	return models.SessionDigest{
		TotalTurns:    len(resp.Events),
		ToolCallCount: len(resp.ToolCalls),
		ToolsUsed:     toolsUsed,
		Errors:        []string{},
	}
}

func (r *TestRunner) buildTranscript(resp *execution.ExecutionResponse) []models.TranscriptEvent {
	entries := make([]models.TranscriptEvent, 0, len(resp.Events))
	for _, evt := range resp.Events {
		entries = append(entries, models.TranscriptEvent{SessionEvent: evt})
	}
	return entries
}

func (r *TestRunner) computeTestStats(runs []models.RunResult) *models.TestStats {
	if len(runs) == 0 {
		return nil
	}

	passed := 0
	totalScore := 0.0
	totalWeightedScore := 0.0
	minScore := math.Inf(1)
	maxScore := math.Inf(-1)
	totalDuration := int64(0)
	scores := make([]float64, 0, len(runs))

	for _, run := range runs {
		score := run.ComputeRunScore()
		weightedScore := run.ComputeWeightedRunScore()
		totalScore += score
		totalWeightedScore += weightedScore
		scores = append(scores, score)

		if score < minScore {
			minScore = score
		}
		if score > maxScore {
			maxScore = score
		}

		if run.AllValidationsPassed() {
			passed++
		}

		totalDuration += run.DurationMs
	}

	stdDev := models.ComputeStdDev(scores)

	stats := &models.TestStats{
		PassRate:         float64(passed) / float64(len(runs)),
		AvgScore:         totalScore / float64(len(runs)),
		AvgWeightedScore: totalWeightedScore / float64(len(runs)),
		MinScore:         minScore,
		MaxScore:         maxScore,
		StdDevScore:      stdDev,
		ScoreVariance:    stdDev * stdDev,
		AvgDurationMs:    totalDuration / int64(len(runs)),
	}

	// Populate flaky, CI, and bootstrap stats for multi-trial runs
	stats.Flaky = stats.PassRate > 0 && stats.PassRate < 1

	if len(runs) >= 2 {
		// Collect weighted scores for bootstrap
		weightedScores := make([]float64, 0, len(runs))
		for _, run := range runs {
			weightedScores = append(weightedScores, run.ComputeWeightedRunScore())
		}

		ci := statistics.BootstrapCI(weightedScores, 0.95)
		stats.BootstrapCI = &ci
		stats.CI95Lo = ci.Lower
		stats.CI95Hi = ci.Upper

		sig := statistics.IsSignificant(ci)
		stats.IsSignificant = &sig
	}

	return stats
}

func (r *TestRunner) buildOutcome(testOutcomes []models.TestOutcome, startTime time.Time) *models.EvaluationOutcome {
	spec := r.cfg.Spec()

	// Compute digest
	succeeded := 0
	failed := 0
	errors := 0

	for _, to := range testOutcomes {
		switch to.Status {
		case models.StatusPassed:
			succeeded++
		case models.StatusFailed:
			failed++
		case models.StatusError:
			errors++
		}
	}

	totalTests := len(testOutcomes)
	successRate := 0.0
	if totalTests > 0 {
		successRate = float64(succeeded) / float64(totalTests)
	}

	// Compute aggregate score, min, max, and stddev across tests
	aggregateScore := r.computeAggregateScore(testOutcomes)
	weightedScore := r.computeWeightedAggregateScore(testOutcomes)
	digestMin, digestMax, digestStdDev := r.computeDigestScoreStats(testOutcomes)

	// Compute group stats if grouping is configured
	groupStats := computeGroupStats(testOutcomes)

	digest := models.OutcomeDigest{
		TotalTests:     totalTests,
		Succeeded:      succeeded,
		Failed:         failed,
		Errors:         errors,
		Skipped:        0,
		SuccessRate:    successRate,
		AggregateScore: aggregateScore,
		WeightedScore:  weightedScore,
		MinScore:       digestMin,
		MaxScore:       digestMax,
		StdDev:         digestStdDev,
		DurationMs:     time.Since(startTime).Milliseconds(),
		Groups:         groupStats,
	}

	// Compute digest-level bootstrap CI over per-test weighted scores when multi-trial
	if spec.Config.RunsPerTest > 1 && len(testOutcomes) > 0 {
		perTestScores := make([]float64, 0, len(testOutcomes))
		for _, to := range testOutcomes {
			if to.Stats != nil {
				perTestScores = append(perTestScores, to.Stats.AvgWeightedScore)
			}
		}
		if len(perTestScores) >= 2 {
			ci := statistics.BootstrapCI(perTestScores, 0.95)
			sig := statistics.IsSignificant(ci)
			digest.Statistics = &models.StatisticalSummary{
				BootstrapCI:   ci,
				IsSignificant: sig,
			}
		}
	}

	return &models.EvaluationOutcome{
		RunID:       fmt.Sprintf("run-%d", time.Now().Unix()),
		SkillTested: spec.SkillName,
		BenchName:   spec.Name,
		Timestamp:   startTime,
		Setup: models.OutcomeSetup{
			RunsPerTest: spec.Config.RunsPerTest,
			ModelID:     spec.Config.ModelID,
			EngineType:  spec.Config.EngineType,
			TimeoutSec:  spec.Config.TimeoutSec,
			JudgeModel:  spec.Config.JudgeModel,
		},
		Digest:       digest,
		Measures:     make(map[string]models.MeasureResult),
		TestOutcomes: testOutcomes,
		Metadata:     make(map[string]any),
	}
}

// computeAggregateScore returns the mean of per-test average scores.
// Tests with nil Stats are treated as having an average score of 0.0.
func (r *TestRunner) computeAggregateScore(testOutcomes []models.TestOutcome) float64 {
	if len(testOutcomes) == 0 {
		return 0.0
	}

	totalScore := 0.0
	for _, to := range testOutcomes {
		if to.Stats != nil {
			totalScore += to.Stats.AvgScore
		}
	}

	return totalScore / float64(len(testOutcomes))
}

// computeWeightedAggregateScore returns the mean of per-test weighted scores.
func (r *TestRunner) computeWeightedAggregateScore(testOutcomes []models.TestOutcome) float64 {
	if len(testOutcomes) == 0 {
		return 0.0
	}

	totalScore := 0.0
	for _, to := range testOutcomes {
		if to.Stats != nil {
			totalScore += to.Stats.AvgWeightedScore
		}
	}

	return totalScore / float64(len(testOutcomes))
}

// computeDigestScoreStats returns min, max, and stddev of per-test average scores.
func (r *TestRunner) computeDigestScoreStats(testOutcomes []models.TestOutcome) (float64, float64, float64) {
	if len(testOutcomes) == 0 {
		return 0.0, 0.0, 0.0
	}

	scores := make([]float64, 0, len(testOutcomes))
	minScore := 1.0
	maxScore := 0.0

	for _, to := range testOutcomes {
		s := 0.0
		if to.Stats != nil {
			s = to.Stats.AvgScore
		}
		scores = append(scores, s)
		if s < minScore {
			minScore = s
		}
		if s > maxScore {
			maxScore = s
		}
	}

	return minScore, maxScore, models.ComputeStdDev(scores)
}

// resolveGroup returns the group value for the current benchmark configuration.
// Currently only "model" is supported; CSV column grouping will be added with #187.
func (r *TestRunner) resolveGroup() string {
	spec := r.cfg.Spec()
	switch spec.Config.GroupBy {
	case "model":
		return spec.Config.ModelID
	case "":
		return ""
	default:
		fmt.Printf("[WARN] unknown group_by value %q, grouping disabled\n", spec.Config.GroupBy)
		return ""
	}
}

// computeGroupStats aggregates per-group statistics from test outcomes.
func computeGroupStats(outcomes []models.TestOutcome) []models.GroupStats {
	type accumulator struct {
		passed     int
		total      int
		scoreTotal float64
		scoreCount int
	}

	groups := make(map[string]*accumulator)
	var order []string

	for _, to := range outcomes {
		if to.Group == "" {
			continue
		}
		acc, exists := groups[to.Group]
		if !exists {
			acc = &accumulator{}
			groups[to.Group] = acc
			order = append(order, to.Group)
		}
		acc.total++
		if to.Status == models.StatusPassed {
			acc.passed++
		}
		if to.Stats != nil {
			acc.scoreTotal += to.Stats.AvgScore
			acc.scoreCount++
		}
	}

	if len(groups) == 0 {
		return nil
	}

	result := make([]models.GroupStats, 0, len(order))
	for _, name := range order {
		acc := groups[name]
		avg := 0.0
		if acc.scoreCount > 0 {
			avg = acc.scoreTotal / float64(acc.scoreCount)
		}
		result = append(result, models.GroupStats{
			Name:     name,
			Passed:   acc.passed,
			Total:    acc.total,
			AvgScore: avg,
		})
	}
	return result
}
