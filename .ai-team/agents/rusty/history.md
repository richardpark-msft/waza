# History â€” Rusty

## Project Context
- **Project:** waza â€” CLI tool for evaluating Agent Skills
- **Stack:** Go (primary), React 19 + Tailwind CSS v4 (web UI)
- **User:** Shayne Boyer (spboyer)
- **Repo:** spboyer/waza
- **Universe:** The Usual Suspects

## Key Learnings

### Architecture
- **Model selection directive (2026-02-18):** Coding in Claude Opus 4.6, reviews in GPT-5.3-Codex, design in Gemini Pro 3
- **Web UI styling:** Keep clean and functional â€” colors close to DevEx dashboard, no fancy gradients
- **Agent execution:** Go engine drives CLI, web UI for visualization

### Code Quality
- Test coverage is non-negotiable
- Interface-based design for flexibility (AgentEngine, Validator patterns in Go)
- Functional options for configuration (Go convention)

### Team Structure
- Linus owns Go backend implementation
- Basher owns all testing strategy
- Livingston/Saul own documentation
- Richard Park available for Copilot SDK questions

## Work Log

### 2026-02-19: #80 â€” BPE Tokenizer (PR #260)
- **Reviewed PR** by Charles Lowell (chlowell) â€” ported BPE tokenizer from Microsoft/Tokenizer
- **Architecture:** `Counter` interface preserved. `NewCounter(tokenizer)` factory replaces `NewEstimatingCounter()`. BPE is new default via `TokenizerDefault`.
- **New package:** `internal/tokens/bpe/` â€” BinaryMap, LRU cache, byte-pair encoder, tokenizer, builder
- **Embedded model:** `o200k_base.tiktoken` (~3.6MB) via `go:embed` â€” adds to binary size
- **Flag design:** `--tokenizer` flag only on `count` command; `check`/`compare`/`suggest` hardcode `TokenizerDefault`
- **Findings:** `regex` field on Tokenizer struct is dead code (set but never read); `NewTokenizerFromFile` is dead code (defined but never called); `Cache` field is exported unnecessarily; LRU cache is not thread-safe (fine for CLI but should be documented)
- **Verdict:** APPROVE with comments â€” architecture is sound, implementation correct, concerns are improvements not blockers

### 2025-07-25: #238 â€” True trajectory replay viewer (PR #243)
- **Branch:** `squad/238-trajectory-viewer`
- Full rewrite of `TrajectoryViewer.tsx` to consume real `TranscriptEvent` data
- Created `SessionDigestCard.tsx` (digest stats + tools used badges + errors)
- Created `ToolCallDetail.tsx` (expandable JSON viewers for args/result)
- Timeline: color-coded dots (blue=tool start, green/red=complete, emerald=turn, red=error)
- `toolCallId` correlation links Start â†” Complete events
- Graceful fallback to grader-based heuristic when transcript is empty
- Depends on #237 (transcript + session digest in API)

## Learnings

### Eval & Grader Registry Architecture (Issues #385â€“#390, Feb 25)

**Key Design Decisions:**
- **Repos-as-registry, not a central JSON file.** A Git repo with `waza.registry.yaml` IS a waza module. Same model as Go modules â€” zero infrastructure to publish. Shayne explicitly rejected single-file registries.
- **Federated index for discovery, not resolution.** Discovery is optional (like GOPROXY). Resolution is always direct Git clone. Offline-first, air-gap friendly.
- **OCI artifacts rejected for Phase 1.** Overkill for YAML+script packages. Docker concepts don't belong in eval authoring UX. Can revisit as alternative transport later.
- **External programs before WASM.** `program` grader already exists. Enhance it with `waza-grader-v1` protocol (stdin JSON â†’ stdout JSON). WASM waits for WASI ecosystem maturity. Go plugins are dead â€” don't use them.
- **OpenAI battle.yaml doesn't map 1:1.** It's a completion-era pattern (compare two outputs). Agent-native equivalent is `waza compare` across runs. Don't cargo-cult completion patterns into agent eval.
- **`choice_scores` on prompt grader, not a new grader type.** One grader type handles classification, CoT, and scoring. Composability > type proliferation.
- **`extends:` uses shallow merge, local wins.** Predictable over clever. Deep merge of eval configs causes surprises.
- **Lock file with SHA-256 hashes.** Supply chain security is non-negotiable for a registry. `waza.lock` committed to source control, verified on every run.

**Document:** `tmp/waza-eval-registry-design.md`

### Release Infrastructure Audit (Issue #223, Feb 20)

**Version Management State:**
- `version.txt` (source-of-truth candidate): 0.4.0-alpha.1
- `extension.yaml`: 0.3.0 (STALE, 2 patch releases behind)
- `registry.json`: max version 0.3.0 (missing 0.4.0-alpha.1, blocking extension users)
- Go binary version: injected via ldflags `-X main.version=${VERSION}` during build

**Key Files & Their Purpose:**
- `.github/workflows/go-release.yml` â€” Standalone CLI release (builds 6 platform matrix, creates GitHub Release on `v*` tag) â€” **ACTIVE, currently used**
- `.github/workflows/azd-ext-release.yml` â€” Extension release (triggers on version.txt/extension.yaml changes, publishes to azd registry via `azd x publish`) â€” **ACTIVE, currently used**
- `.github/workflows/release.yml` â€” NEW unified workflow (both CLI + extension from single trigger, includes version sync job) â€” **PREPARED BUT NOT YET ACTIVATED**
- `.github/workflows/squad-release.yml` â€” Package.json-based release (NOT relevant to waza-go)
- `Makefile` â€” Local build with `VERSION?=0.1.0` (default, overridable)
- `build.sh` â€” Cross-platform binary builder for extension (VERSION env var driven)
- `cmd/waza/root.go` â€” Version variable: `var version = "dev"` (overwritten at build-time)

**Critical Issues Identified:**
1. **No unified release trigger** â€” CLI and extension release independently, easy to desync
2. **Version sync failure** â€” extension.yaml not bumped when CLI version.txt is updated
3. **registry.json desynchronization** â€” stale, depends on manual azd-ext-release workflow execution
4. **release.yml has logical flaw** â€” sync-versions job runs AFTER build jobs, too late to affect artifact versions
5. **Dual tag schemes** â€” CLI uses `v*`, extension uses `azd-ext-microsoft-azd-waza_*`, confusing
6. **No pre-flight validation** â€” easy for files to drift; no check before build

**Recommended Architecture:**
- Single canonical trigger: git tag `v*.*.*` 
- release.yml should be the sole release coordinator (retire go-release.yml + azd-ext-release.yml once stable)
- Pre-flight job in release.yml to validate version.txt == tag before proceeding
- Sync-versions job should run BEFORE builds, not after
- Document the flow in docs/RELEASE.md

**Immediate Action:** registry.json is blocking extension users on 0.4.0-alpha.1. Should manually trigger release.yml or azd-ext-release.yml to sync.


## ðŸ“Œ Team update (2026-02-20): Model policy overhaul

All code roles now use `claude-opus-4.6`. Docs/Scribe/diversity use `gemini-3-pro-preview`. Heavy code gen uses `gpt-5.2-codex`. Decided by Scott Boyer. See decisions.md for full details.

### Competitive Analysis: Waza vs OpenAI Evals (Feb 21)

**Key Findings:**
- Waza wins 10 of 15 comparison dimensions on shipped capability. The fundamental gap is architectural: OpenAI Evals evaluates *completions*, waza evaluates *agents*. Different paradigm entirely.
- OpenAI Evals' strengths are ecosystem (800+ community evals, brand recognition, LangChain integration) and custom eval extensibility (Python class inheritance). Waza's strengths are agent-native architecture, trajectory visualization, statistical rigor, and development toolchain.
- OpenAI's Platform dashboard (platform.openai.com) is vendor-locked to OpenAI models â€” not a competitive threat to waza's model-agnostic approach.
- Highest-impact moves: (1) positioning doc framing "agent eval vs LLM eval" narrative, (2) migration guide from OpenAI Evals format, (3) grader plugin interface to close extensibility gap.
- OpenAI Evals cannot evaluate agents without a ground-up redesign. As the industry moves from completions to agents, this becomes a legacy framework for a shrinking use case.

**Document:** `tmp/waza-vs-openai-evals.md`

### Competitive Analysis: Waza vs skill-validator (Feb 25)

**Key Findings:**
- skill-validator is the closest peer competitor â€” same SDK (Copilot SDK), same domain (agent skill eval), same eval format (YAML scenarios). Not a legacy framework like OpenAI Evals; this is a real threat.
- Waza wins 9 of 15 dimensions (platform depth, visualization, multi-model, grader variety, data-driven testing, caching, MCP, distribution, hooks). skill-validator wins 4 (A/B testing, pairwise judging, tool constraints, auto-discovery). 2 at parity.
- skill-validator's 4 wins are concentrated in one coherent insight: **A/B testing (with/without skill)**. This answers "does the skill *help*?" â€” a question waza cannot answer with absolute grading alone. This is the single most important idea to adopt.
- Pairwise comparative judging with position-swap bias mitigation is architecturally more rigorous than waza's independent `prompt` grader for relative quality assessment.
- Tool constraint assertions (`expect_tools`/`reject_tools`) are a real gap â€” waza captures tool calls in trajectories but has no grader that inspects them.
- The strategic move: adopt skill-validator's A/B methodology (`--baseline` mode, pairwise judging, tool constraints) while maintaining platform advantages. Moving right on the "validates improvement" axis is easier than skill-validator moving up on the "full lifecycle" axis.
- Worst-case scenario: skill-validator becomes the "correct" way to validate skills internally at Microsoft while waza is seen as "just an eval runner." Best-case: waza ships `--baseline` before skill-validator ships a dashboard.

**Document:** `tmp/waza-vs-skill-validator.md`
