# Waza User Guide

Welcome to **Waza**, a CLI tool for evaluating AI agent skills. This guide covers everything you need to get started: installation, creating evaluations, running benchmarks, and reviewing results in the dashboard.

## What is Waza?

Waza helps you:

- **Define agent skills** with comprehensive documentation and behavioral requirements
- **Create test suites** with realistic test cases and validation rules  
- **Run evaluations** against different AI models to measure skill effectiveness
- **Compare results** across models and versions to track improvement
- **View metrics** in an interactive dashboard with live results, trends, and detailed analysis

Perfect for skill authors, platform teams, and developers building AI-powered applications.

---

## Installation

Choose one of three methods:

### 1. Binary Install (Recommended)

The fastest way to get started. The script auto-detects your OS and architecture:

```bash
curl -fsSL https://raw.githubusercontent.com/spboyer/waza/main/install.sh | bash
```

This downloads the latest release, verifies the checksum, and installs the `waza` binary to:
- `/usr/local/bin/waza` (if writable), or
- `~/bin/waza` (if `/usr/local/bin` is not writable)

After installation, verify it works:

```bash
waza --version
```

**Note:** If installed to `~/bin`, add it to your PATH:

```bash
export PATH="$HOME/bin:$PATH"
```

### 2. Install from Source

Requires **Go 1.26 or later**:

```bash
go install github.com/spboyer/waza/cmd/waza@latest
```

Verify installation:

```bash
waza --version
```

### 3. Azure Developer CLI Extension

If you use Azure Developer CLI (`azd`), install waza as an extension:

```bash
# Add the waza extension registry
azd ext source add -n waza -t url -l https://raw.githubusercontent.com/spboyer/waza/main/registry.json

# Install the extension
azd ext install microsoft.azd.waza

# Verify it works
azd waza --help
```

All waza commands are available under `azd waza`:

```bash
azd waza init my-project
azd waza run eval.yaml
azd waza serve
```

---

## Quick Start

Get a complete evaluation suite running in 5 minutes.

### Step 1: Initialize a Project

Create a new directory and initialize a waza project:

```bash
mkdir my-eval-suite
cd my-eval-suite
waza init
```

You'll be prompted to create your first skill. Enter a name like `code-explainer`, and the scaffolding is automatic. This creates:

```
my-eval-suite/
â”œâ”€â”€ skills/                     # Skill definitions
â”‚   â””â”€â”€ code-explainer/
â”‚       â””â”€â”€ SKILL.md            # Skill metadata and description
â”œâ”€â”€ evals/                      # Evaluation suites
â”‚   â””â”€â”€ code-explainer/
â”‚       â”œâ”€â”€ eval.yaml           # Evaluation configuration
â”‚       â”œâ”€â”€ tasks/              # Test case definitions
â”‚       â”‚   â”œâ”€â”€ basic-usage.yaml
â”‚       â”‚   â””â”€â”€ edge-cases.yaml
â”‚       â””â”€â”€ fixtures/           # Test data and resources
â”‚           â””â”€â”€ sample.py
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ eval.yml                # CI/CD pipeline
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

**Skip the prompt?** Use `--no-skill`:

```bash
waza init --no-skill
```

### Step 2: Create a New Skill (if needed)

If you didn't create one during `init`, add a new skill:

```bash
waza new code-analyzer
```

This scaffolds a new skill with SKILL.md and eval suite. The interactive wizard will collect metadata (name, description, use cases).

### Step 3: Define Your Evaluation

Edit `evals/code-explainer/eval.yaml`:

```yaml
name: code-explainer
description: "Tests the agent's ability to explain code"
model: claude-sonnet-4.6
maxTokens: 4096
tasks:
  - id: basic-usage
    description: "Explain a simple Python function"
    fixture: sample.py
    expectedOutput:
      - contains: "function"
      - contains: "parameter"
      - regex: "returns.*value"
validators:
  - type: contains
    caseSensitive: false
  - type: regex
    caseSensitive: false
```

Add test case YAML files in `tasks/`:

```yaml
# tasks/basic-usage.yaml
id: basic-usage
description: "Explain a simple Python function"
prompt: "Explain what this function does:\n{{fixture:sample.py}}"
expectedOutput:
  - type: contains
    value: "function"
  - type: regex
    value: "returns.*value"
tags: ["basic", "core"]
```

Add fixtures in `fixtures/`:

```python
# fixtures/sample.py
def greet(name):
    """Return a greeting message."""
    return f"Hello, {name}!"
```

### Step 4: Run the Evaluation

Execute the benchmark:

```bash
waza run evals/code-explainer/eval.yaml --context-dir evals/code-explainer/fixtures -v
```

**Output:**
- `âœ“ Passed` â€” Task passed all validators
- `âœ— Failed` â€” Task failed one or more validators
- Summary: `X passed, Y failed`

Save results to a JSON file:

```bash
waza run evals/code-explainer/eval.yaml \
  --context-dir evals/code-explainer/fixtures \
  -o results.json
```

### Step 5: View Results in the Dashboard

Launch the web dashboard:

```bash
waza serve
```

The browser opens automatically to `http://localhost:3000`. You'll see:

- **Dashboard Overview** â€” Run history, pass rate, model comparison
- **Run Details** â€” Individual task results, validation output
- **Compare** â€” Side-by-side model performance
- **Trends** â€” Pass rate over time

---

## Command Reference

### `waza init [directory]`

Initialize a waza project with directory structure.

**Flags:**
- `--no-skill` â€” Skip the first-skill creation prompt

**What it creates:**
- `skills/` â€” Skill definitions directory
- `evals/` â€” Evaluation suites directory
- `.github/workflows/eval.yml` â€” CI/CD pipeline
- `.gitignore` â€” With waza-specific entries
- `README.md` â€” Getting started guide

**Example:**
```bash
waza init my-skills --no-skill
```

---

### `waza new [skill-name]`

Create a new skill with its evaluation suite.

**Flags:**
- `--template, -t` â€” Template pack (coming soon)

**Modes:**

**Project mode** (inside a `skills/` directory):
```bash
cd my-skills-repo
waza new code-explainer
```
Creates `skills/code-explainer/SKILL.md` and `evals/code-explainer/`.

**Standalone mode** (no `skills/` directory):
```bash
cd my-project
waza new my-skill
```
Creates `my-skill/` with `SKILL.md`, `evals/`, CI/CD pipeline, and README.

**Examples:**
```bash
# Interactive wizard in a terminal
waza new code-analyzer

# Non-interactive (CI/CD)
waza new code-analyzer << EOF
Code Analyzer
Analyzes code for patterns and issues
code, analysis
EOF
```

---

### `waza run [eval.yaml | skill-name]`

Run an evaluation benchmark.

**Arguments:**
- `[eval.yaml]` â€” Path to evaluation spec file
- `[skill-name]` â€” Skill name (auto-detects eval.yaml)
- *(none)* â€” Auto-detect using workspace detection

**Flags:**
- `--context-dir` â€” Fixtures directory (default: `./fixtures`)
- `--output, -o` â€” Save results JSON
- `--verbose, -v` â€” Detailed progress output
- `--parallel` â€” Run tasks concurrently
- `--workers <n>` â€” Number of concurrent workers (default: 4)
- `--task <pattern>` â€” Filter tasks by name (glob pattern, can repeat)
- `--tags <pattern>` â€” Filter tasks by tags (glob pattern, can repeat)
- `--model <model>` â€” Override model (can repeat for multi-model runs)
- `--cache` â€” Enable result caching
- `--no-cache` â€” Disable caching (default)
- `--cache-dir <path>` â€” Cache directory (default: `.waza-cache`)
- `--format <format>` â€” Output format: `default`, `github-comment`
- `--interpret` â€” Print plain-language interpretation of results
- `--baseline` â€” Run A/B comparison: with skills vs without

**Examples:**

Run an evaluation from a spec file:
```bash
waza run evals/code-explainer/eval.yaml --context-dir evals/code-explainer/fixtures -v
```

Run a specific skill:
```bash
waza run code-explainer
```

Auto-detect in workspace:
```bash
# Single-skill workspace â†’ runs that skill's eval
# Multi-skill workspace â†’ runs all evals with summary
waza run
```

Run specific tasks:
```bash
waza run evals/code-explainer/eval.yaml --task "basic*" --task "edge*"
```

Run with specific models:
```bash
waza run evals/code-explainer/eval.yaml --model "gpt-4o" --model "claude-sonnet-4.6"
```

Save results:
```bash
waza run evals/code-explainer/eval.yaml -o results.json
```

---

### `waza check [skill-name | skill-path]`

Validate that a skill is ready for submission.

**Arguments:**
- `[skill-name]` â€” Skill name (e.g., `code-explainer`)
- `[skill-path]` â€” Path to skill directory (e.g., `skills/my-skill`)
- *(none)* â€” Auto-detect using workspace detection

**What it checks:**
1. **Compliance scoring** â€” Validates SKILL.md frontmatter (Low/Medium/Medium-High/High)
2. **Token budget** â€” Ensures SKILL.md is within limits
3. **Evaluation presence** â€” Confirms eval.yaml exists

**Output:**
- âœ“ Compliance level (e.g., "Medium-High")
- âœ“ Token count / limit
- âœ“ Evaluation status
- Suggestions for improvement

**Examples:**

Check a specific skill:
```bash
waza check code-explainer
```

Check by path:
```bash
waza check skills/my-skill
```

Auto-detect in workspace:
```bash
# Single skill â†’ checks that skill
# Multiple skills â†’ summary table for all
waza check
```

---

### `waza serve`

Launch the web dashboard to view evaluation results.

**Flags:**
- `--port <n>` â€” HTTP server port (default: `3000`)
- `--no-browser` â€” Don't auto-open the browser
- `--results-dir <path>` â€” Directory to read results from (default: `.`)
- `--tcp <address>` â€” JSON-RPC TCP server (e.g., `:9000`)
- `--tcp-allow-remote` â€” Bind to all interfaces (default: loopback only)
- `--http` â€” Start HTTP dashboard (default)

**Dashboard Pages:**
- **Overview** â€” Run history, pass rates, model comparison
- **Run Details** â€” Individual task results, validation logs, timestamps
- **Compare** â€” Side-by-side model performance
- **Trends** â€” Pass rate and timing trends over time
- **Live View** â€” Real-time results during active runs

**Examples:**

Start the dashboard:
```bash
waza serve
```

Custom port:
```bash
waza serve --port 8080
```

Load results from a specific directory:
```bash
waza serve --results-dir ./archived-runs
```

Don't auto-open browser:
```bash
waza serve --no-browser
```

JSON-RPC TCP server (for IDE integration):
```bash
waza serve --tcp :9000
```

---

## Advanced Usage

### Caching and Reproducibility

Cache evaluation results to avoid redundant runs:

```bash
# Enable caching (results stored in .waza-cache/)
waza run evals/code-explainer/eval.yaml --cache -o results.json
```

The cache stores results by task ID and model, keyed on the prompt and expected output. Same inputs = same cached results.

**Use cases:**
- Development â€” Iterate on validators without re-running tasks
- CI/CD â€” Avoid redundant API calls for unchanged evals
- Offline â€” Use cached results when connection is unavailable

Disable caching:
```bash
waza run evals/code-explainer/eval.yaml --no-cache
```

Specify cache location:
```bash
waza run evals/code-explainer/eval.yaml --cache --cache-dir ./my-cache
```

---

### Filtering and Selective Runs

Run only specific tasks:

```bash
# By task name (glob patterns)
waza run evals/code-explainer/eval.yaml --task "basic*" --task "edge*"

# By tag
waza run evals/code-explainer/eval.yaml --tags "critical"
```

**Task names and tags** are defined in your task YAML files:

```yaml
# tasks/basic-usage.yaml
id: basic-usage
description: "Explain a simple function"
tags: ["basic", "core"]
```

---

### Parallel Execution

Run tasks concurrently to speed up evaluation:

```bash
# Use default 4 workers
waza run evals/code-explainer/eval.yaml --parallel

# Use custom worker count
waza run evals/code-explainer/eval.yaml --parallel --workers 8
```

**When to use parallel execution:**
- Many tasks (50+)
- Long-running validators
- Development machines with spare CPU

---

### Multi-Model Comparison

Run the same evaluation against multiple models:

```bash
waza run evals/code-explainer/eval.yaml \
  --model "gpt-4o" \
  --model "claude-sonnet-4.6" \
  --model "gpt-4-turbo"
```

Results are grouped by model. Compare them in the dashboard or with:

```bash
waza compare results-gpt4.json results-sonnet.json
```

---

### CI/CD Integration

Run waza as part of your GitHub Actions workflow:

```yaml
# .github/workflows/eval.yml
name: Evaluate Skills

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-go@v4
        with:
          go-version: '1.26'
      - name: Install waza
        run: |
          curl -fsSL https://raw.githubusercontent.com/spboyer/waza/main/install.sh | bash
      - name: Run evals
        run: |
          waza run --output results.json -v
      - name: Comment PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('results.json'));
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `Evaluation Results: ${results.passCount}/${results.totalCount} passed`
            });
```

---

### Session Logging

Capture detailed session logs for debugging:

```bash
waza run evals/code-explainer/eval.yaml --session-log --session-dir ./logs
```

Logs are stored in NDJSON format (one event per line):
```json
{"event":"task_started","id":"basic-usage","timestamp":"2024-01-15T10:30:00Z"}
{"event":"task_completed","id":"basic-usage","passed":true,"timestamp":"2024-01-15T10:30:05Z"}
```

---

### Output Formats

**Default format** (human-readable):
```bash
waza run evals/code-explainer/eval.yaml
```

**GitHub Comment format** (for PR comments):
```bash
waza run evals/code-explainer/eval.yaml --format github-comment
```

**JSON output** (for automation):
```bash
waza run evals/code-explainer/eval.yaml -o results.json
```

---

## Dashboard

The `waza serve` command launches an interactive web dashboard for viewing and analyzing evaluation results.

### Starting the Dashboard

```bash
waza serve
```

The dashboard opens automatically at `http://localhost:3000`.

### Navigation

- **Overview** â€” Evaluation summary, pass rates, recent runs
- **Run Details** â€” Click a run to see task-by-task results
- **Compare** â€” Select multiple runs to compare models
- **Trends** â€” Historical pass rates and performance over time
- **Live View** â€” Real-time results during active evaluations

### Dashboard Features

- **Live updates** â€” See results as tasks complete
- **Search** â€” Find runs, tasks, and models
- **Filtering** â€” Filter by status (passed/failed), tags, date range
- **Export** â€” Download results as CSV or JSON
- **Dark mode** â€” Switch to dark theme for comfortable viewing

### Advanced Dashboard Flags

**Custom port:**
```bash
waza serve --port 8080
```

**Skip auto-open:**
```bash
waza serve --no-browser
```

**Load results from archive:**
```bash
waza serve --results-dir ./previous-runs
```

**JSON-RPC for IDE integration:**
```bash
waza serve --tcp :9000
```

---

## Troubleshooting

### Port Already in Use

If you see "address already in use," the default port 3000 is taken.

**Solution:** Use a different port:
```bash
waza serve --port 3001
```

Or find and stop the process using port 3000:
```bash
# macOS/Linux
lsof -i :3000
kill -9 <PID>

# Windows
netstat -ano | findstr :3000
taskkill /PID <PID> /F
```

---

### No Results Found in Dashboard

If the dashboard shows no results:

1. **Check results directory:** Make sure you've saved results with `-o`:
   ```bash
   waza run evals/code-explainer/eval.yaml -o results.json
   ```

2. **Specify results directory:** If results are in a subdirectory:
   ```bash
   waza serve --results-dir ./eval-results
   ```

3. **Verify file format:** Results must be JSON files (`*.json`).

---

### Fixture Directory Not Found

If you see "fixture directory not found":

1. **Check the path:** Is your `--context-dir` correct?
   ```bash
   waza run eval.yaml --context-dir ./evals/fixtures
   ```

2. **Use relative paths:** Path is relative to the spec file:
   ```
   eval.yaml
   fixtures/
   ```

3. **From any directory:** Specify absolute paths:
   ```bash
   waza run /path/to/eval.yaml --context-dir /path/to/fixtures
   ```

---

### Validation Failures

If tasks fail validation unexpectedly:

1. **Enable verbose output:** See what the model returned:
   ```bash
   waza run eval.yaml -v
   ```

2. **Check your validators:** Are the rules too strict?
   ```yaml
   validators:
     - type: contains
       value: "function"
       caseSensitive: false  # Don't be too strict
   ```

3. **Adjust expected output:** Make rules realistic:
   ```yaml
   expectedOutput:
     - type: contains
       value: "important keyword"  # Not every word needs to match
   ```

---

## Next Steps

- **[Create your first skill](./GETTING-STARTED.md)** â€” Step-by-step walkthrough
- **[Skill best practices](./SKILL-BEST-PRACTICES.md)** â€” Guidelines for effective skills
- **[Validators reference](./GRADERS.md)** â€” All validator types and options
- **[Token limits](./TOKEN-LIMITS.md)** â€” Optimize SKILL.md for size
- **[Examples](../examples/)** â€” Real eval suites to explore

---

## Getting Help

- **[GitHub Issues](https://github.com/spboyer/waza/issues)** â€” Report bugs or request features
- **[Discussions](https://github.com/spboyer/waza/discussions)** â€” Ask questions, share ideas
- **[Waza Examples](../examples/)** â€” Runnable evaluation suites

---

**Happy evaluating! ðŸš€**
